@phdthesis{antunes-2015,
  author    = {Antunes, Diego Roberto},
  title     = {Proposta de um modelo computacional para representa{\c{c}}{\~{a}}o
               de sinais em uma arquitetura de servi{\c{c}}os {HCI-SL} para l{\'{i}}nguas
               de sinais},
  school    = {Federal University of Paran{\'{a}}, Curitiba, Brazil},
  year      = {2015},
  url       = {http://dspace.c3sl.ufpr.br:8080/dspace/handle/1884/41015}
}

@mastersthesis{antunes-2011,
  author    = {Antunes, Diego Roberto},
  title     = {Um modelo de descri{\c{c}}{\~{a}}o computacional da fonologia da l\'ingua de sinais brasileira},
  school    = {Federal University of Paran{\'{a}}, Curitiba, Brazil},
  year      = {2011},
  url       = {https://acervodigital.ufpr.br/handle/1884/25642}
}

@book{quadros-2004,
  title     ={L{\'{i}}ngua de sinais brasileira: estudos lingu{\'{i}}sticos},
  author    ={Quadros, Ronice M{\"{u}}ller de and 
                Karnopp, Lodenir Becker},
  publisher ={Artmed},
  address   ={Porto Alegre },
  volume    ={1},
  year      ={2004}
}

@book{pereira-choi-2011,
  title     ={Libras - Conhecimento Al{\'{e}}m Dos Sinais},
  author    ={Pereira, Maria Cristina da Cunha and 
                Choi, Daniel and 
                Vieira, Maria In{\^{e}}s and 
                Gaspar, Priscilla and 
                Nakasato, Ricardo},
  isbn      ={9788576058786},
  publisher ={Pearson},
  year      ={2011},
  edition   ={1},
  address   ={S{\~{a}}o Paulo}
}

@inproceedings{antunes-hcisl-2011,
  author    = {Antunes, Diego Roberto and
               Guimar{\~{a}}es, Cayley and
               Garc{\'{i}}a, Laura S{\'{a}}nchez and
               Oliveira, Luiz S. and
               Fernandes, Sueli},
  title     = {A framework to support development of Sign Language human-computer
               interaction: Building tools for effective information access and inclusion
               of the deaf},
  booktitle = {Proceedings of the Fifth {IEEE} International Conference on Research
               Challenges in Information Science, {RCIS} 2011, Gosier, Guadeloupe,
               France, 19-21 May, 2011},
  pages     = {1--12},
  year      = {2011},
  crossref  = {rcis-2011},
  url       = {https://doi.org/10.1109/RCIS.2011.6006832},
  doi       = {10.1109/RCIS.2011.6006832},
  timestamp = {Mon, 06 Nov 2017 12:14:30 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/rcis/AntunesGGOF11},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@proceedings{rcis-2011,
  title     = {Proceedings of the Fifth {IEEE} International Conference on Research
               Challenges in Information Science, {RCIS} 2011, Gosier, Guadeloupe,
               France, 19-21 May, 2011},
  publisher = {{IEEE}},
  year      = {2011},
  url       = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=5992815},
  isbn      = {978-1-4244-8670-0},
  timestamp = {Thu, 22 Sep 2011 08:19:06 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/rcis/2011},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{athitsos-asllvd-2008, 
    author      = {V. Athitsos and C. Neidle and S. Sclaroff and J. Nash and A. Stefan and Quan Yuan and A. Thangali}, 
    booktitle   = {2008 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}, 
    title       = {The American Sign Language Lexicon Video Dataset}, 
    year        = {2008}, 
    volume      = {}, 
    number      = {}, 
    pages       = {1-8}, 
    keywords    = {computer vision;handicapped aids;image sequences;learning (artificial intelligence);American sign language lexicon video dataset;written representation;English translation;public dataset;video sequences;computer vision system;machine learning methods;Handicapped aids;Dictionaries;Video sequences;Vocabulary;Computer science;Computer vision;Data engineering;Learning systems;Design methodology;Machine learning}, 
    doi         = {10.1109/CVPRW.2008.4563181}, 
    ISSN        = {2160-7508}, 
    month       = {06}
}

@inproceedings{cao-realtime-2017,
  author 	= {Zhe Cao and Tomas Simon and Shih-En Wei and Yaser Sheikh},
  booktitle = {CVPR},
  title 	= {Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields},
  year      = {2017}
}

@inproceedings{simon-hand-2017,
  author    = {Tomas Simon and Hanbyul Joo and Iain Matthews and Yaser Sheikh},
  booktitle = {CVPR},
  title     = {Hand Keypoint Detection in Single Images using Multiview Bootstrapping},
  year      = {2017}
}

@inproceedings{wei-cpm-2016,
  author    = {Shih-En Wei and Varun Ramakrishna and Takeo Kanade and Yaser Sheikh},
  booktitle = {CVPR},
  title     = {Convolutional pose machines},
  year      = {2016}
}

@book{goodfellow-2016,
    title       = {Deep Learning},
    author      = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher   = {MIT Press},
    note        = {\url{http://www.deeplearningbook.org}},
    year        = {2016}
}

@article{st-gcn-2018,
  author    = {Sijie Yan and
               Yuanjun Xiong and
               Dahua Lin},
  title     = {Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition},
  journal   = {CoRR},
  volume    = {abs/1801.07455},
  year      = {2018},
  url       = {http://arxiv.org/abs/1801.07455},
  archivePrefix = {arXiv},
  eprint    = {1801.07455},
  timestamp = {Mon, 13 Aug 2018 16:48:19 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-07455},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{garcia-2013,
    author = {Sánchez García, Laura and Guimarães, Cayley and Antunes, Diego and Fernandes, Sueli},
    year = {2013},
    month = {07},
    pages = {},
    title = {HCI Architecture for Deaf Communities Cultural Inclusion and Citizenship},
    volume = {3},
    booktitle = {ICEIS 2013 - Proceedings of the 15th International Conference on Enterprise Information Systems}
}

@INPROCEEDINGS{wang-2012, 
author={J. Wang and Z. Liu and Y. Wu and J. Yuan}, 
booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
title={Mining actionlet ensemble for action recognition with depth cameras}, 
year={2012}, 
volume={}, 
number={}, 
pages={1290-1297}, 
keywords={cameras;image motion analysis;image recognition;image sensors;object tracking;actionlet ensemble mining;human action recognition;commodity depth sensor;depth map;depth camera;3D position;tracked joints;occlusion;intraclass variation;actionlet ensemble model;human motion;human-object interaction;MoCap system;Joints;Humans;Hidden Markov models;Cameras;Robustness;Noise;Feature extraction}, 
doi={10.1109/CVPR.2012.6247813}, 
ISSN={1063-6919}, 
month={June},}


@INPROCEEDINGS{fernando-2015, 
author={B. Fernando and E. Gavves and M. José Oramas and A. Ghodrati and T. Tuytelaars}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Modeling video evolution for action recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={5378-5387}, 
keywords={gesture recognition;image motion analysis;image representation;object recognition;video signal processing;video evolution;action recognition;video-wide temporal information;ranking functions;ranking machine;video representation;generic action recognition;fine-grained actions;gestures;Hollywood2;MDB51;MPII-cooking activities;Chalearn;local motion based methods;Hidden Markov models;Training;Trajectory;Support vector machines;Encoding;Object recognition;Robustness}, 
doi={10.1109/CVPR.2015.7299176}, 
ISSN={1063-6919}, 
month={June},}

@article{shahroudy-2016,
  author    = {Amir Shahroudy and
               Jun Liu and
               Tian{-}Tsong Ng and
               Gang Wang},
  title     = {{NTU} {RGB+D:} {A} Large Scale Dataset for 3D Human Activity Analysis},
  journal   = {CoRR},
  volume    = {abs/1604.02808},
  year      = {2016},
  url       = {http://arxiv.org/abs/1604.02808},
  archivePrefix = {arXiv},
  eprint    = {1604.02808},
  timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ShahroudyLNW16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@INPROCEEDINGS{yong-du-2015, 
author={Yong Du and W. Wang and L. Wang}, 
booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
title={Hierarchical recurrent neural network for skeleton based action recognition}, 
year={2015}, 
volume={}, 
number={}, 
pages={1110-1118}, 
keywords={feature extraction;image motion analysis;image representation;image sequences;perceptrons;recurrent neural nets;hierarchical recurrent neural network;end-to-end hierarchical RNN;skeleton based action recognition;subnet representation extraction;single-layer perceptron;deep RNN architectures;temporal sequences;Joints;Hidden Markov models;Recurrent neural networks;Computer architecture;Artificial neural networks;Neurons}, 
doi={10.1109/CVPR.2015.7298714}, 
ISSN={1063-6919}, 
month={June},}

@online{openpose-output-2018,
  author = {{CMU Perceptual Computing Lab}},
  title = {{OpenPose} Demo - Output},
  year = 2018,
  url = {https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/doc/output.md},
  urldate = {2018-11-15}
}



@inproceedings{neidle-2012,
    author = {Neidle, Carol and Thangali, Ashwin and Sclaroff, Stan},
    year = {2012},
    month = {05},
    pages = {},
    title = {Challenges in Development of the American Sign Language Lexicon Video Dataset ({ASLLVD}) Corpus},
    URL = {https://open.bu.edu/handle/2144/31899},
    booktitle = {5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, LREC 2012, Istanbul, Turkey}
}

@inproceedings{vloger-2012,
    author = {Vogler, Christian and Neidle, Carol},
    year = {2012},
    month = {05},
    pages = {},
    title = {A new web interface to facilitate access to corpora: development of the {ASLLRP} data access interface},
    URL = {https://open.bu.edu/handle/2144/31886},
    booktitle = {Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, LREC 2012, Istanbul, Turkey}
}

@book{gallaudet-2005,
  title={The Gallaudet Dictionary of American Sign Language},
  author={Valli, C. and Gallaudet University},
  number={v. 1},
  isbn={9781563682827},
  lccn={2005051129},
  series={The Gallaudet Dictionary of American Sign Language},
  year={2005},
  publisher={Gallaudet University Press}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@techreport{python,
 author = {Rossum, Guido},
 title = {Python Reference Manual},
 year = {1995},
 source = {http://www.ncstrl.org:8900/ncstrl/servlet/search?formname=detail\&id=oai%3Ancstrlh%3Aercim_cwi%3Aercim.cwi%2F%2FCS-R9525},
 institution = {CWI (Centre for Mathematics and Computer Science)},
 address = {Amsterdam, The Netherlands, The Netherlands},
} 



@INPROCEEDINGS{recent-advances-dl-2017, 
    author={L. Zheng and B. Liang and A. Jiang}, 
    booktitle={2017 International Conference on Digital Image Computing: Techniques and Applications (DICTA)}, 
    title={Recent Advances of Deep Learning for Sign Language Recognition}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={1-7}, 
    keywords={cameras;feature extraction;gesture recognition;handicapped aids;learning (artificial intelligence);sign language recognition;vocabulary;advanced machine learning technologies;deep learning;sign language recognition;social interaction;gesture recognition;low-cost depth camera;action recognition;interactive communication tools;feature extraction;depth sensors;classification;finger spelling;vocabulary words;Gesture recognition;Assistive technology;Sensors;Cameras;Feature extraction;Machine learning;Training}, 
    doi={10.1109/DICTA.2017.8227483}, 
    ISSN={}, 
    month={Nov}
}

@INPROCEEDINGS{recent-advances-sl-2013, 
    author={M. F. Tolba and A. S. Elons}, 
    booktitle={2013 8th International Conference on Computer Engineering Systems (ICCES)}, 
    title={Recent developments in sign language recognition systems}, 
    year={2013}, 
    volume={}, 
    number={}, 
    pages={xxxvi-xlii}, 
    keywords={human computer interaction;natural language processing;sign language recognition;speech recognition;lab research;industrial product;Arabic sign language;visual sign language;speaking people;speech-recognition systems;hearing-impaired individual;nonvocal community;physically challenged individuals;automated translation systems;sign language recognition systems;Assistive technology;Gesture recognition;Hidden Markov models;Cameras;Sensors;Three-dimensional displays;Biological neural networks;Arabic Sign Language (ArSL);hand gesture;hand posture;movement transition}, 
    doi={10.1109/ICCES.2013.6707157}, 
    ISSN={}, 
    month={Nov}
}

@article{bhof-sl-2016,
    title = "Block-based histogram of optical flow for isolated sign language recognition",
    journal = "Journal of Visual Communication and Image Representation",
    volume = "40",
    pages = "538 - 545",
    year = "2016",
    issn = "1047-3203",
    doi = "https://doi.org/10.1016/j.jvcir.2016.07.020",
    url = "http://www.sciencedirect.com/science/article/pii/S1047320316301468",
    author = "Kian Ming Lim and Alan W.C. Tan and Shing Chiang Tan",
    keywords = "Sign language recognition, Block-based, Histogram of optical flow",
    abstract = "In this paper, we propose a block-based histogram of optical flow (BHOF) to generate hand representation in sign language recognition. Optical flow of the sign language video is computed in a region centered around the location of the detected hand position. The hand patches of optical flow are segmented into M spatial blocks, where each block is a cuboid of a segment of a frame across the entire sign gesture video. The histogram of each block is then computed and normalized by its sum. The feature vector of all blocks are then concatenated as the BHOF sign gesture representation. The proposed method provides a compact scale-invariant representation of the sign language. Furthermore, block-based histogram encodes spatial information and provides local translation invariance in the extracted optical flow. Additionally, the proposed BHOF also introduces sign language length invariancy into its representation, and thereby, produce promising recognition rate in signer independent problems."
}

@INPROCEEDINGS{shanta-2018, 
    author={S. S. Shanta and S. T. Anwar and M. R. Kabir}, 
    booktitle={2018 9th International Conference on Computing, Communication and Networking Technologies (ICCCNT)}, 
    title={Bangla Sign Language Detection Using SIFT and CNN}, 
    year={2018}, 
    volume={}, 
    number={}, 
    pages={1-6}, 
    keywords={convolution;feature extraction;feedforward neural nets;image classification;natural language processing;sign language recognition;transforms;CNN;Bangla sign language system;SIFT feature extraction;Bangla sign language detection;convolutional neural network;classification;Bangla Sign Language;SIFT;CNN;BoF}, 
    doi={10.1109/ICCCNT.2018.8493915}, 
    ISSN={}, 
    month={July}
}

@Article{sift-2004,
    author="Lowe, David G.",
    title="Distinctive Image Features from Scale-Invariant Keypoints",
    journal="International Journal of Computer Vision",
    year="2004",
    month="Nov",
    day="01",
    volume="60",
    number="2",
    pages="91--110",
    abstract="This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a substantial range of affine distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and finally performing verification through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.",
    issn="1573-1405",
    doi="10.1023/B:VISI.0000029664.99615.94",
    url="https://doi.org/10.1023/B:VISI.0000029664.99615.94"
}

@INPROCEEDINGS{hog-2005, 
    author={N. Dalal and B. Triggs}, 
    booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)}, 
    title={Histograms of oriented gradients for human detection}, 
    year={2005}, 
    volume={1}, 
    number={}, 
    pages={886-893 vol. 1}, 
    keywords={object detection;support vector machines;object recognition;feature extraction;gradient methods;histograms of oriented gradients;human detection;robust visual object recognition;linear SVM;edge based descriptors;gradient based descriptors;fine-scale gradients;fine orientation binning;coarse spatial binning;contrast normalization;overlapping descriptor;pedestrian database;Histograms;Humans;Robustness;Object recognition;Support vector machines;Object detection;Testing;Image edge detection;High performance computing;Image databases}, 
    doi={10.1109/CVPR.2005.177}, 
    ISSN={1063-6919}, 
    month={June}
}

@INPROCEEDINGS{hof-2008, 
    author={I. Laptev and M. Marszalek and C. Schmid and B. Rozenfeld}, 
    booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
    title={Learning realistic human actions from movies}, 
    year={2008}, 
    volume={}, 
    number={}, 
    pages={1-8}, 
    keywords={cinematography;image classification;image retrieval;learning (artificial intelligence);support vector machines;video signal processing;video realistic human action recognition;movie script;automatic video annotation;human action retrieval;text-based classifier;visual learning;video action classification;local space-time feature;space-time pyramid;multichannel nonlinear SVM;Humans;Motion pictures;Image recognition;Video sharing;Layout;Text categorization;Object recognition;Robustness;Clothing;Cameras}, 
    doi={10.1109/CVPR.2008.4587756}, 
    ISSN={1063-6919}, 
    month={June},
}

@INPROCEEDINGS{ji-2017, 
    author={Y. Ji and S. Kim and K. Lee}, 
    booktitle={2017 First IEEE International Conference on Robotic Computing (IRC)}, 
    title={Sign Language Learning System with Image Sampling and Convolutional Neural Network}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={371-375}, 
    keywords={computer aided instruction;handicapped aids;neural nets;sign language recognition;sign language learning system;image sampling;convolutional neural network;novel sign language learning system;2D image sampling;conventional sign recognition;sign language demonstration video;learning process;sign language actions;Assistive technology;Gesture recognition;Neural networks;Training data;Conferences;Two dimensional displays;Image recognition;sign language;video recognition;machine learning;CNN}, 
    doi={10.1109/IRC.2017.40}, 
    ISSN={}, 
    month={April},
}

@INPROCEEDINGS{taskiran-2018,
    author = {Taskiran, Murat and Kıllıoğlu, Mehmet and Kahraman, Nihan},
    year = {2018},
    month = {07},
    pages = {},
    title = {A Real-Time System For Recognition of American Sign Language by Using Deep Learning},
    doi = {10.1109/TSP.2018.8441304}
}

@INPROCEEDINGS{rao-2018, 
    author={G. A. Rao and K. Syamala and P. V. V. Kishore and A. S. C. S. Sastry}, 
    booktitle={2018 Conference on Signal Processing And Communication Engineering Systems (SPACES)}, 
    title={Deep convolutional neural networks for sign language recognition}, 
    year={2018}, 
    volume={}, 
    number={}, 
    pages={194-197}, 
    keywords={computer vision;convolution;feedforward neural nets;handicapped aids;mobile computing;sign language recognition;video signal processing;SLR mobile application;mobile selfie sign language;CNN training;different CNN architectures;selfie sign language data;convolutional neural networks;sign language recognition;computer vision;Indian sign language gestures;powerful artificial intelligence tool;selfie mode continuous sign language video;hearing-impaired person;Training;Assistive technology;Gesture recognition;Testing;Stochastic processes;Computer architecture;Feature extraction;Selfie sign language;Convolutional Neural Networks (CNN);Stochastic pooling;Sign recognition;Deep learning}, 
    doi={10.1109/SPACES.2018.8316344}, 
    ISSN={}, 
    month={Jan},
}

@INPROCEEDINGS{elbadawy-2017, 
    author={M. ElBadawy and A. S. Elons and H. A. Shedeed and M. F. Tolba}, 
    booktitle={2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS)}, 
    title={Arabic sign language recognition with 3D convolutional neural networks}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={66-71}, 
    keywords={convolution;feature extraction;handicapped aids;human computer interaction;natural language processing;neural nets;pattern classification;sign language recognition;3D convolutional neural networks;features extractor;3D Convolutional Neural Network;Arabic sign language dictionary;Arabic Sign Language Recognition;Hearing Impaired people;classifiers;Conferences;Information systems;Arabic Sign Language;Deep models;Convolutional Neural Network;Sign Language Recognition}, 
    doi={10.1109/INTELCIS.2017.8260028}, 
    ISSN={}, 
    month={Dec},
}

@INPROCEEDINGS{das-2018, 
    author={A. Das and S. Gawde and K. Suratwala and D. Kalbande}, 
    booktitle={2018 International Conference on Smart City and Emerging Technology (ICSCET)}, 
    title={Sign Language Recognition Using Deep Learning on Custom Processed Static Gesture Images}, 
    year={2018}, 
    volume={}, 
    number={}, 
    pages={1-6}, 
    keywords={cameras;convolution;feature extraction;feedforward neural nets;image classification;image filtering;learning (artificial intelligence);sign language recognition;deep learning;sign language detection;image classification;machine learning;sign language gestures;static sign language;convolutional neural network model;static gesture images;sign language recognition;RGB camera;convolution filter;Assistive technology;Gesture recognition;Testing;Convolutional neural networks;Training;Feature extraction;Sociology;Sign Language Recognition;Deep Learning;Inception V3;Image Processing}, 
    doi={10.1109/ICSCET.2018.8537248}, 
    ISSN={}, 
    month={Jan},
}

@INPROCEEDINGS{sajanraj-2018, 
    author={T. D. Sajanraj and M. Beena}, 
    booktitle={2018 Second International Conference on Inventive Communication and Computational Technologies (ICICCT)}, 
    title={Indian Sign Language Numeral Recognition Using Region of Interest Convolutional Neural Network}, 
    year={2018}, 
    volume={}, 
    number={}, 
    pages={636-640}, 
    keywords={convolution;feature extraction;feedforward neural nets;handicapped aids;image colour analysis;image segmentation;learning (artificial intelligence);natural language processing;Python;sign language recognition;classifier model;RGB camera system;Indian Sign Language numeral recognition;deaf community;real-time system;deep learning approach;convolutional neural network;numeral signs;region segmentation;region of interest convolutional neural network;ISL;handcrafted feature;sign classification;Keras implementation;python;skin segmentation;low light condition;Feature extraction;Gesture recognition;Assistive technology;Convolutional neural networks;Real-time systems;Image segmentation;Image color analysis;Deep learning;Convolutional neural network;Region of interest;Real-time system}, 
    doi={10.1109/ICICCT.2018.8473141}, 
    ISSN={}, 
    month={April},
}

@INPROCEEDINGS{konstantinidis-2018, 
    author={D. Konstantinidis and K. Dimitropoulos and P. Daras}, 
    booktitle={2018 - 3DTV-Conference: The True Vision - Capture, Transmission and Display of 3D Video (3DTV-CON)}, 
    title={SIGN LANGUAGE RECOGNITION BASED ON HAND AND BODY SKELETAL DATA}, 
    year={2018}, 
    volume={}, 
    number={}, 
    pages={1-4}, 
    keywords={computer vision;feature extraction;gesture recognition;handicapped aids;image colour analysis;learning (artificial intelligence);sign language recognition;sign language recognition;body skeletal features;gesture recognition skeletal data;sign language dataset;hand and body skeletal data;deep learning-based methodology;Sign language;deep learning;linear dynamic system;skeletal data}, 
    doi={10.1109/3DTV.2018.8478467}, 
    ISSN={2161-203X}, 
    month={June},
}

@INPROCEEDINGS{pigou-2017, 
    author={L. Pigou and M. V. Herreweghe and J. Dambre}, 
    booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)}, 
    title={Gesture and Sign Language Recognition with Temporal Residual Networks}, 
    year={2017}, 
    volume={}, 
    number={}, 
    pages={3086-3093}, 
    keywords={feature extraction;learning (artificial intelligence);neural nets;sign language recognition;video streaming;Corpus NGT;Flemish Sign Language Corpus;Corpus VGT;ChaLearn LAP RGB-D Continuous Gesture Dataset;temporal residual networks;sign language recognition;continuous video stream;framewise classification problem;temporal convolutions;Dutch Sign Language Corpus;batch normalization;exponential linear units;mean Jaccard index;ChaLearn LAP ConGD;depth maps;vocabulary;Assistive technology;Gesture recognition;Training;Neural networks;Adaptation models;Feature extraction}, 
    doi={10.1109/ICCVW.2017.365}, 
    ISSN={2473-9944}, 
    month={Oct},
}